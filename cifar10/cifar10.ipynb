{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me6J0s0WVQh9"
      },
      "source": [
        "This is based on [this article](https://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/)\n",
        "\n",
        "Simple Convolutional Neural Network for CIFAR-10\n",
        "The CIFAR-10 problem is best solved using a Convolutional Neural Network (CNN).\n",
        "\n",
        "We can quickly start off by defining all of the classes and functions we will need in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwLiIOUcVUjO"
      },
      "source": [
        "# Simple CNN model for CIFAR-10\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu3zYehyVcD1"
      },
      "source": [
        "Next we can load the CIFAR-10 dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vqPIJBIVce6"
      },
      "source": [
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd66RolPVlEG"
      },
      "source": [
        "The pixel values are in the range of 0 to 255 for each of the red, green and blue channels.\n",
        "\n",
        "It is good practice to work with normalized data. Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation which is 255.\n",
        "\n",
        "Note, the data is loaded as integers, so we must cast it to floating point values in order to perform the division."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOV5ai1VVmgZ"
      },
      "source": [
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8fXAd0XVyzy"
      },
      "source": [
        "The output variables are defined as a vector of integers from 0 to 1 for each class.\n",
        "\n",
        "We can use a one hot encoding to transform them into a binary matrix in order to best model the classification problem. We know there are 10 classes for this problem, so we can expect the binary matrix to have a width of 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKcVWOSVuHt"
      },
      "source": [
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c2esi9gV66L"
      },
      "source": [
        "Let’s start off by defining a simple CNN structure as a baseline and evaluate how well it performs on the problem.\n",
        "\n",
        "We will use a structure with two convolutional layers followed by max pooling and a flattening out of the network to fully connected layers to make predictions.\n",
        "\n",
        "Our baseline network structure can be summarized as follows:\n",
        "\n",
        "Convolutional input layer, 32 feature maps with a size of 3×3, a rectifier activation function and a weight constraint of max norm set to 3.\n",
        "Dropout set to 20%.\n",
        "Convolutional layer, 32 feature maps with a size of 3×3, a rectifier activation function and a weight constraint of max norm set to 3.\n",
        "Max Pool layer with size 2×2.\n",
        "Flatten layer.\n",
        "Fully connected layer with 512 units and a rectifier activation function.\n",
        "Dropout set to 50%.\n",
        "Fully connected output layer with 10 units and a softmax activation function.\n",
        "A logarithmic loss function is used with the stochastic gradient descent optimization algorithm configured with a large momentum and weight decay start with a learning rate of 0.01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pefx8DFWAl0",
        "outputId": "e05be035-ff67-4f81-cf80-214ff1b7bac3"
      },
      "source": [
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# Compile model\n",
        "epochs = 25\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               4194816   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 4,210,090\n",
            "Trainable params: 4,210,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF8QFflxWYCw"
      },
      "source": [
        "We can fit this model with 25 epochs and a batch size of 32.\n",
        "\n",
        "A small number of epochs was chosen to help keep this tutorial moving. Normally the number of epochs would be one or two orders of magnitude larger for this problem.\n",
        "\n",
        "Once the model is fit, we evaluate it on the test dataset and print out the classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzssKPMDWavt",
        "outputId": "6109ba5a-8821-45ac-8062-2443957f1897"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 226s 144ms/step - loss: 1.6866 - accuracy: 0.3917 - val_loss: 1.3944 - val_accuracy: 0.4967\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 1.3462 - accuracy: 0.5169 - val_loss: 1.2097 - val_accuracy: 0.5679\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 220s 141ms/step - loss: 1.1645 - accuracy: 0.5847 - val_loss: 1.0790 - val_accuracy: 0.6222\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 1.0371 - accuracy: 0.6336 - val_loss: 1.0178 - val_accuracy: 0.6443\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 0.9322 - accuracy: 0.6700 - val_loss: 0.9740 - val_accuracy: 0.6578\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 220s 141ms/step - loss: 0.8454 - accuracy: 0.7030 - val_loss: 0.9514 - val_accuracy: 0.6671\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 222s 142ms/step - loss: 0.7733 - accuracy: 0.7269 - val_loss: 0.9195 - val_accuracy: 0.6791\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 0.7086 - accuracy: 0.7502 - val_loss: 0.9054 - val_accuracy: 0.6859\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 221s 141ms/step - loss: 0.6451 - accuracy: 0.7735 - val_loss: 0.9226 - val_accuracy: 0.6858\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 221s 141ms/step - loss: 0.5919 - accuracy: 0.7916 - val_loss: 0.9121 - val_accuracy: 0.6867\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 217s 139ms/step - loss: 0.5457 - accuracy: 0.8090 - val_loss: 0.9348 - val_accuracy: 0.6917\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 220s 141ms/step - loss: 0.5001 - accuracy: 0.8234 - val_loss: 0.9160 - val_accuracy: 0.6965\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 222s 142ms/step - loss: 0.4642 - accuracy: 0.8359 - val_loss: 0.9396 - val_accuracy: 0.6948\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 222s 142ms/step - loss: 0.4299 - accuracy: 0.8495 - val_loss: 0.9486 - val_accuracy: 0.7002\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 0.4029 - accuracy: 0.8575 - val_loss: 0.9581 - val_accuracy: 0.7030\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 218s 139ms/step - loss: 0.3679 - accuracy: 0.8713 - val_loss: 0.9855 - val_accuracy: 0.6977\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 218s 140ms/step - loss: 0.3500 - accuracy: 0.8772 - val_loss: 0.9927 - val_accuracy: 0.7020\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 0.3261 - accuracy: 0.8856 - val_loss: 1.0022 - val_accuracy: 0.6979\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 0.3071 - accuracy: 0.8917 - val_loss: 1.0123 - val_accuracy: 0.7000\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 0.2832 - accuracy: 0.9004 - val_loss: 1.0444 - val_accuracy: 0.7024\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 228s 146ms/step - loss: 0.2673 - accuracy: 0.9073 - val_loss: 1.0354 - val_accuracy: 0.7054\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 225s 144ms/step - loss: 0.2549 - accuracy: 0.9117 - val_loss: 1.0443 - val_accuracy: 0.7038\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 225s 144ms/step - loss: 0.2407 - accuracy: 0.9177 - val_loss: 1.0686 - val_accuracy: 0.7042\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 225s 144ms/step - loss: 0.2303 - accuracy: 0.9209 - val_loss: 1.0790 - val_accuracy: 0.7088\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 225s 144ms/step - loss: 0.2165 - accuracy: 0.9253 - val_loss: 1.0941 - val_accuracy: 0.7063\n",
            "Accuracy: 70.63%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX-nqTfrXFEO"
      },
      "source": [
        "## Larger Convolutional Neural Network for CIFAR-10\n",
        "We have seen that a simple CNN performs poorly on this complex problem. In this section we look at scaling up the size and complexity of our model.\n",
        "\n",
        "Let’s design a deep version of the simple CNN above. We can introduce an additional round of convolutions with many more feature maps. We will use the same pattern of Convolutional, Dropout, Convolutional and Max Pooling layers.\n",
        "\n",
        "This pattern will be repeated 3 times with 32, 64, and 128 feature maps. The effect be an increasing number of feature maps with a smaller and smaller size given the max pooling layers. Finally an additional and larger Dense layer will be used at the output end of the network in an attempt to better translate the large number feature maps to class values.\n",
        "\n",
        "We can summarize a new network architecture as follows:\n",
        "\n",
        "Convolutional input layer, 32 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "Dropout layer at 20%.\n",
        "Convolutional layer, 32 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "Max Pool layer with size 2×2.\n",
        "Convolutional layer, 64 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "Dropout layer at 20%.\n",
        "Convolutional layer, 64 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "Max Pool layer with size 2×2.\n",
        "Convolutional layer, 128 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "Dropout layer at 20%.\n",
        "Convolutional layer,128 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "Max Pool layer with size 2×2.\n",
        "Flatten layer.\n",
        "Dropout layer at 20%.\n",
        "Fully connected layer with 1024 units and a rectifier activation function.\n",
        "Dropout layer at 20%.\n",
        "Fully connected layer with 512 units and a rectifier activation function.\n",
        "Dropout layer at 20%.\n",
        "Fully connected output layer with 10 units and a softmax activation function.\n",
        "We can very easily define this network topology in Keras, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwIYSx63XJKz",
        "outputId": "64bdea77-c74e-4169-e3e7-71a000bb968f"
      },
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# Compile model\n",
        "epochs = 25\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 2,915,114\n",
            "Trainable params: 2,915,114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw7tdTgEXM6O"
      },
      "source": [
        "We can fit and evaluate this model using the same a procedure above and the same number of epochs but a larger batch size of 64, found through some minor experimentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amBz8RloXT7H",
        "outputId": "c4ca9d52-0478-4a0f-f75c-0f9fa899b932"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  5/782 [..............................] - ETA: 5:39 - loss: 2.3042 - accuracy: 0.1094"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}